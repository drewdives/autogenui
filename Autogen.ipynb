{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/drewdives/autogenui/blob/main/Autogen.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gcz_5h7une7H",
        "outputId": "c1f63fb4-4d8e-4acc-b930-74dcd07aad36"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pyautogen\n",
            "  Downloading pyautogen-0.2.26-py3-none-any.whl (264 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m264.9/264.9 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting diskcache (from pyautogen)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docker (from pyautogen)\n",
            "  Downloading docker-7.0.0-py3-none-any.whl (147 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.6/147.6 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting flaml (from pyautogen)\n",
            "  Downloading FLAML-2.1.2-py3-none-any.whl (296 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m296.7/296.7 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from pyautogen) (1.25.2)\n",
            "Collecting openai<1.21,>=1.3 (from pyautogen)\n",
            "  Downloading openai-1.20.0-py3-none-any.whl (292 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m292.8/292.8 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic!=2.6.0,<3,>=1.10 in /usr/local/lib/python3.10/dist-packages (from pyautogen) (2.7.0)\n",
            "Collecting python-dotenv (from pyautogen)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from pyautogen) (2.4.0)\n",
            "Collecting tiktoken (from pyautogen)\n",
            "  Downloading tiktoken-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai<1.21,>=1.3->pyautogen) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai<1.21,>=1.3->pyautogen) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai<1.21,>=1.3->pyautogen)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai<1.21,>=1.3->pyautogen) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai<1.21,>=1.3->pyautogen) (4.66.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai<1.21,>=1.3->pyautogen) (4.11.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=2.6.0,<3,>=1.10->pyautogen) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=2.6.0,<3,>=1.10->pyautogen) (2.18.1)\n",
            "Requirement already satisfied: packaging>=14.0 in /usr/local/lib/python3.10/dist-packages (from docker->pyautogen) (24.0)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from docker->pyautogen) (2.31.0)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from docker->pyautogen) (2.0.7)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken->pyautogen) (2023.12.25)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<1.21,>=1.3->pyautogen) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<1.21,>=1.3->pyautogen) (1.2.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai<1.21,>=1.3->pyautogen) (2024.2.2)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai<1.21,>=1.3->pyautogen)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai<1.21,>=1.3->pyautogen)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->docker->pyautogen) (3.3.2)\n",
            "Installing collected packages: python-dotenv, h11, flaml, diskcache, tiktoken, httpcore, docker, httpx, openai, pyautogen\n",
            "Successfully installed diskcache-5.6.3 docker-7.0.0 flaml-2.1.2 h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 openai-1.20.0 pyautogen-0.2.26 python-dotenv-1.0.1 tiktoken-0.6.0\n"
          ]
        }
      ],
      "source": [
        "!pip install pyautogen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M9dMGnjmbx-L"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "from autogen import ConversableAgent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kSdLm26qnePC"
      },
      "outputs": [],
      "source": [
        "# Set your OpenAI API key here\n",
        "os.environ['OPENAI_API_KEY'] = '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'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eESWCF2gnFps"
      },
      "outputs": [],
      "source": [
        "\n",
        "agent = ConversableAgent(\n",
        "    \"chatbot\",\n",
        "    llm_config={\"config_list\": [{\"model\": \"gpt-4\", \"api_key\": os.environ.get(\"OPENAI_API_KEY\")}]},\n",
        "    code_execution_config=False,  # Turn off code execution, by default it is off.\n",
        "    function_map=None,  # No registered functions, by default it is None.\n",
        "    human_input_mode=\"NEVER\",  # Never ask for human input.\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vaC97n5NnM2O",
        "outputId": "aef33761-78b7-4840-8ea6-6bbbd6d0a8ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sure, here's a classic one for you:\n",
            "\n",
            "Why don't scientists trust atoms?\n",
            "\n",
            "Because they make up everything!\n"
          ]
        }
      ],
      "source": [
        "reply = agent.generate_reply(messages=[{\"content\": \"Tell me a joke.\", \"role\": \"user\"}])\n",
        "print(reply)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xD-lvJRCnRln"
      },
      "source": [
        "# Roles and Conversations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "77L1S1JAnTte"
      },
      "outputs": [],
      "source": [
        "cathy = ConversableAgent(\n",
        "    \"cathy\",\n",
        "    system_message=\"Your name is Cathy and you are a part of a duo of comedians.\",\n",
        "    llm_config={\"config_list\": [{\"model\": \"gpt-4\", \"temperature\": 0.9, \"api_key\": os.environ.get(\"OPENAI_API_KEY\")}]},\n",
        "    human_input_mode=\"NEVER\",  # Never ask for human input.\n",
        ")\n",
        "\n",
        "joe = ConversableAgent(\n",
        "    \"joe\",\n",
        "    system_message=\"Your name is Joe and you are a part of a duo of comedians.\",\n",
        "    llm_config={\"config_list\": [{\"model\": \"gpt-4\", \"temperature\": 0.7, \"api_key\": os.environ.get(\"OPENAI_API_KEY\")}]},\n",
        "    human_input_mode=\"NEVER\",  # Never ask for human input.\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ncGWP9PnYen",
        "outputId": "1faee745-8eab-4fc4-f475-5712a04c896e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "joe (to cathy):\n",
            "\n",
            "Cathy, tell me a joke.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "cathy (to joe):\n",
            "\n",
            "Sure, here's a classic one: \n",
            "\n",
            "Why don't we ever tell secrets on a farm?\n",
            "\n",
            "Because the potatoes have eyes, the corn has ears, and the beans stalk!\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "joe (to cathy):\n",
            "\n",
            "That's a good one, Cathy! Now here's one from me:\n",
            "\n",
            "Why don't scientists trust atoms?\n",
            "\n",
            "Because they make up everything!\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "cathy (to joe):\n",
            "\n",
            "Haha, that's a great one! Never trust an atom, they really do make up everything! You've got a knack for science humor.\n",
            "\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "result = joe.initiate_chat(cathy, message=\"Cathy, tell me a joke.\", max_turns=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sAUflRiAnpoP"
      },
      "source": [
        "#Terminating Conversations Between Agents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rn8yHP58n4TP",
        "outputId": "60b0c3a2-9037-4402-8c25-9c53cd3c5a75"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "joe (to cathy):\n",
            "\n",
            "Cathy, tell me a joke.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "cathy (to joe):\n",
            "\n",
            "Sure, here's a classic one: \n",
            "\n",
            "Why don't we ever tell secrets on a farm?\n",
            "\n",
            "Because the potatoes have eyes, the corn has ears, and the beans stalk!\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "joe (to cathy):\n",
            "\n",
            "That's a good one, Cathy! Now here's one from me:\n",
            "\n",
            "Why don't scientists trust atoms?\n",
            "\n",
            "Because they make up everything!\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "cathy (to joe):\n",
            "\n",
            "Haha, that's a great one! Never trust an atom, they really do make up everything! You've got a knack for science humor.\n",
            "\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "joe = ConversableAgent(\n",
        "    \"joe\",\n",
        "    system_message=\"Your name is Joe and you are a part of a duo of comedians.\",\n",
        "    llm_config={\"config_list\": [{\"model\": \"gpt-4\", \"temperature\": 0.7, \"api_key\": os.environ.get(\"OPENAI_API_KEY\")}]},\n",
        "    human_input_mode=\"NEVER\",  # Never ask for human input.\n",
        "    max_consecutive_auto_reply=1,  # Limit the number of consecutive auto-replies.\n",
        ")\n",
        "\n",
        "result = joe.initiate_chat(cathy, message=\"Cathy, tell me a joke.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A34qPqkRn8nM",
        "outputId": "349f148e-f4b7-4c82-a88d-bb493a9de91a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "joe (to cathy):\n",
            "\n",
            "Cathy, tell me a joke and then say the words GOOD BYE.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "cathy (to joe):\n",
            "\n",
            "Sure, here's a classic for you:\n",
            "\n",
            "Why don't scientists trust atoms?\n",
            "\n",
            "Because they make up everything!\n",
            "\n",
            "GOOD BYE!\n",
            "\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "joe = ConversableAgent(\n",
        "    \"joe\",\n",
        "    system_message=\"Your name is Joe and you are a part of a duo of comedians.\",\n",
        "    llm_config={\"config_list\": [{\"model\": \"gpt-4\", \"temperature\": 0.7, \"api_key\": os.environ.get(\"OPENAI_API_KEY\")}]},\n",
        "    human_input_mode=\"NEVER\",  # Never ask for human input.\n",
        "    is_termination_msg=lambda msg: \"good bye\" in msg[\"content\"].lower(),\n",
        ")\n",
        "\n",
        "result = joe.initiate_chat(cathy, message=\"Cathy, tell me a joke and then say the words GOOD BYE.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fBQcTD8Yp-i6"
      },
      "source": [
        "#Allowing Human Feedback in Agents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y-DTE6SXp_gR",
        "outputId": "d65f6958-fc18-48ed-fc56-b3a7e650ca47"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "agent_with_number (to agent_guess_number):\n",
            "\n",
            "I have a number between 1 and 100. Guess it!\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "agent_guess_number (to agent_with_number):\n",
            "\n",
            "Okay. Is it 50?\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "agent_with_number (to agent_guess_number):\n",
            "\n",
            "Too low.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "agent_guess_number (to agent_with_number):\n",
            "\n",
            "Is it 75?\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "agent_with_number (to agent_guess_number):\n",
            "\n",
            "Too high.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "agent_guess_number (to agent_with_number):\n",
            "\n",
            "Is it 63?\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "agent_with_number (to agent_guess_number):\n",
            "\n",
            "Too high.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "agent_guess_number (to agent_with_number):\n",
            "\n",
            "Is it 57?\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "agent_with_number (to agent_guess_number):\n",
            "\n",
            "Too high.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "agent_guess_number (to agent_with_number):\n",
            "\n",
            "Is it 53?\n",
            "\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "from autogen import ConversableAgent\n",
        "\n",
        "agent_with_number = ConversableAgent(\n",
        "    \"agent_with_number\",\n",
        "    system_message=\"You are playing a game of guess-my-number. You have the \"\n",
        "    \"number 53 in your mind, and I will try to guess it. \"\n",
        "    \"If I guess too high, say 'too high', if I guess too low, say 'too low'. \",\n",
        "    llm_config={\"config_list\": [{\"model\": \"gpt-4\", \"api_key\": os.environ[\"OPENAI_API_KEY\"]}]},\n",
        "    is_termination_msg=lambda msg: \"53\" in msg[\"content\"],  # terminate if the number is guessed by the other agent\n",
        "    human_input_mode=\"NEVER\",  # never ask for human input\n",
        ")\n",
        "\n",
        "agent_guess_number = ConversableAgent(\n",
        "    \"agent_guess_number\",\n",
        "    system_message=\"I have a number in my mind, and you will try to guess it. \"\n",
        "    \"If I say 'too high', you should guess a lower number. If I say 'too low', \"\n",
        "    \"you should guess a higher number. \",\n",
        "    llm_config={\"config_list\": [{\"model\": \"gpt-4\", \"api_key\": os.environ[\"OPENAI_API_KEY\"]}]},\n",
        "    human_input_mode=\"NEVER\",\n",
        ")\n",
        "\n",
        "result = agent_with_number.initiate_chat(\n",
        "    agent_guess_number,\n",
        "    message=\"I have a number between 1 and 100. Guess it!\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XdCxQvjbqFhu",
        "outputId": "6b208b4b-2362-401b-8228-753236d79d89"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "human_proxy (to agent_with_number):\n",
            "\n",
            "10\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "agent_with_number (to human_proxy):\n",
            "\n",
            "Too low.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Provide feedback to agent_with_number. Press enter to skip and use auto-reply, or type 'exit' to end the conversation: 85\n",
            "human_proxy (to agent_with_number):\n",
            "\n",
            "85\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "agent_with_number (to human_proxy):\n",
            "\n",
            "Too high.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Provide feedback to agent_with_number. Press enter to skip and use auto-reply, or type 'exit' to end the conversation: 50\n",
            "human_proxy (to agent_with_number):\n",
            "\n",
            "50\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "agent_with_number (to human_proxy):\n",
            "\n",
            "Too low.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Provide feedback to agent_with_number. Press enter to skip and use auto-reply, or type 'exit' to end the conversation: 62\n",
            "human_proxy (to agent_with_number):\n",
            "\n",
            "62\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "agent_with_number (to human_proxy):\n",
            "\n",
            "Too high.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Provide feedback to agent_with_number. Press enter to skip and use auto-reply, or type 'exit' to end the conversation: 55\n",
            "human_proxy (to agent_with_number):\n",
            "\n",
            "55\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "agent_with_number (to human_proxy):\n",
            "\n",
            "Too high.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Provide feedback to agent_with_number. Press enter to skip and use auto-reply, or type 'exit' to end the conversation: 53\n",
            "human_proxy (to agent_with_number):\n",
            "\n",
            "53\n",
            "\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "human_proxy = ConversableAgent(\n",
        "    \"human_proxy\",\n",
        "    llm_config=False,  # no LLM used for human proxy\n",
        "    human_input_mode=\"ALWAYS\",  # always ask for human input\n",
        ")\n",
        "\n",
        "# Start a chat with the agent with number with an initial guess.\n",
        "result = human_proxy.initiate_chat(\n",
        "    agent_with_number,  # this is the same agent with the number as before\n",
        "    message=\"10\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aNkWpTKTqQuu",
        "outputId": "aced30cf-8ed4-4066-e9db-b8a2f5d29ef1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "agent_with_number (to agent_guess_number):\n",
            "\n",
            "I have a number between 1 and 100. Guess it!\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "agent_guess_number (to agent_with_number):\n",
            "\n",
            "Okay. Is it 50?\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            ">>>>>>>> USING AUTO REPLY...\n",
            "agent_with_number (to agent_guess_number):\n",
            "\n",
            "Too low.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "agent_guess_number (to agent_with_number):\n",
            "\n",
            "Is it 75?\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Please give feedback to agent_guess_number. Press enter to skip and use auto-reply, or type 'exit' to stop the conversation: \n",
            "\n",
            ">>>>>>>> NO HUMAN INPUT RECEIVED.\n",
            "\n",
            ">>>>>>>> USING AUTO REPLY...\n",
            "agent_with_number (to agent_guess_number):\n",
            "\n",
            "Too high.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "agent_guess_number (to agent_with_number):\n",
            "\n",
            "Is it 63?\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Please give feedback to agent_guess_number. Press enter to skip and use auto-reply, or type 'exit' to stop the conversation: yes\n",
            "agent_with_number (to agent_guess_number):\n",
            "\n",
            "yes\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "agent_guess_number (to agent_with_number):\n",
            "\n",
            "Great! I'm glad I could guess your number!\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            ">>>>>>>> USING AUTO REPLY...\n",
            "agent_with_number (to agent_guess_number):\n",
            "\n",
            "That was a mistake on my part. I apologise. The number is too high.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "agent_guess_number (to agent_with_number):\n",
            "\n",
            "No problem at all. Is it 58?\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Please give feedback to agent_guess_number. Press enter to skip and use auto-reply, or type 'exit' to stop the conversation: \n",
            "\n",
            ">>>>>>>> NO HUMAN INPUT RECEIVED.\n",
            "\n",
            ">>>>>>>> USING AUTO REPLY...\n",
            "agent_with_number (to agent_guess_number):\n",
            "\n",
            "Too high.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "agent_guess_number (to agent_with_number):\n",
            "\n",
            "Is it 55?\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Please give feedback to agent_guess_number. Press enter to skip and use auto-reply, or type 'exit' to stop the conversation: \n",
            "\n",
            ">>>>>>>> NO HUMAN INPUT RECEIVED.\n",
            "\n",
            ">>>>>>>> USING AUTO REPLY...\n",
            "agent_with_number (to agent_guess_number):\n",
            "\n",
            "Too high.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "agent_guess_number (to agent_with_number):\n",
            "\n",
            "Is it 52?\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Please give feedback to agent_guess_number. Press enter to skip and use auto-reply, or type 'exit' to stop the conversation: \n",
            "\n",
            ">>>>>>>> NO HUMAN INPUT RECEIVED.\n",
            "\n",
            ">>>>>>>> USING AUTO REPLY...\n",
            "agent_with_number (to agent_guess_number):\n",
            "\n",
            "Too low.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "agent_guess_number (to agent_with_number):\n",
            "\n",
            "Is it 54?\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Please give feedback to agent_guess_number. Press enter to skip and use auto-reply, or type 'exit' to stop the conversation: \n",
            "\n",
            ">>>>>>>> NO HUMAN INPUT RECEIVED.\n",
            "\n",
            ">>>>>>>> USING AUTO REPLY...\n",
            "agent_with_number (to agent_guess_number):\n",
            "\n",
            "Too high.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "agent_guess_number (to agent_with_number):\n",
            "\n",
            "Is it 53?\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Please give feedback to agent_guess_number. Press enter or type 'exit' to stop the conversation: \n",
            "\n",
            ">>>>>>>> NO HUMAN INPUT RECEIVED.\n"
          ]
        }
      ],
      "source": [
        "agent_with_number = ConversableAgent(\n",
        "    \"agent_with_number\",\n",
        "    system_message=\"You are playing a game of guess-my-number. \"\n",
        "    \"In the first game, you have the \"\n",
        "    \"number 53 in your mind, and I will try to guess it. \"\n",
        "    \"If I guess too high, say 'too high', if I guess too low, say 'too low'. \",\n",
        "    llm_config={\"config_list\": [{\"model\": \"gpt-4\", \"api_key\": os.environ[\"OPENAI_API_KEY\"]}]},\n",
        "    max_consecutive_auto_reply=1,  # maximum number of consecutive auto-replies before asking for human input\n",
        "    is_termination_msg=lambda msg: \"53\" in msg[\"content\"],  # terminate if the number is guessed by the other agent\n",
        "    human_input_mode=\"TERMINATE\",  # ask for human input until the game is terminated\n",
        ")\n",
        "\n",
        "agent_guess_number = ConversableAgent(\n",
        "    \"agent_guess_number\",\n",
        "    system_message=\"I have a number in my mind, and you will try to guess it. \"\n",
        "    \"If I say 'too high', you should guess a lower number. If I say 'too low', \"\n",
        "    \"you should guess a higher number. \",\n",
        "    llm_config={\"config_list\": [{\"model\": \"gpt-4\", \"api_key\": os.environ[\"OPENAI_API_KEY\"]}]},\n",
        "    human_input_mode=\"NEVER\",\n",
        ")\n",
        "\n",
        "result = agent_with_number.initiate_chat(\n",
        "    agent_guess_number,\n",
        "    message=\"I have a number between 1 and 100. Guess it!\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I1xUYif9qXFA"
      },
      "source": [
        "#Code Executors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B4BVH2Hmqi_y"
      },
      "outputs": [],
      "source": [
        "import tempfile\n",
        "\n",
        "from autogen import ConversableAgent\n",
        "from autogen.coding import LocalCommandLineCodeExecutor\n",
        "\n",
        "# Create a temporary directory to store the code files.\n",
        "temp_dir = tempfile.TemporaryDirectory()\n",
        "\n",
        "# Create a local command line code executor.\n",
        "executor = LocalCommandLineCodeExecutor(\n",
        "    timeout=10,  # Timeout for each code execution in seconds.\n",
        "    work_dir=temp_dir.name,  # Use the temporary directory to store the code files.\n",
        ")\n",
        "\n",
        "# Create an agent with code executor configuration.\n",
        "code_executor_agent = ConversableAgent(\n",
        "    \"code_executor_agent\",\n",
        "    llm_config=False,  # Turn off LLM for this agent.\n",
        "    code_execution_config={\"executor\": executor},  # Use the local command line code executor.\n",
        "    human_input_mode=\"ALWAYS\",  # Always take human input for this agent for safety.\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3c3_aieYrCRe"
      },
      "outputs": [],
      "source": [
        "! pip install -qqq matplotlib numpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e1yM8FqKrEVQ",
        "outputId": "aa4f1cee-efb6-4c6c-c9c5-241a682d5834"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Provide feedback to the sender. Press enter to skip and use auto-reply, or type 'exit' to end the conversation: \n",
            "\n",
            ">>>>>>>> NO HUMAN INPUT RECEIVED.\n",
            "\n",
            ">>>>>>>> USING AUTO REPLY...\n",
            "\n",
            ">>>>>>>> EXECUTING CODE BLOCK (inferred language is python)...\n",
            "exitcode: 0 (execution succeeded)\n",
            "Code output: Scatter plot saved to scatter.png\n",
            "\n"
          ]
        }
      ],
      "source": [
        "message_with_code_block = \"\"\"This is a message with code block.\n",
        "The code block is below:\n",
        "```python\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "x = np.random.randint(0, 100, 100)\n",
        "y = np.random.randint(0, 100, 100)\n",
        "plt.scatter(x, y)\n",
        "plt.savefig('scatter.png')\n",
        "print('Scatter plot saved to scatter.png')\n",
        "```\n",
        "This is the end of the message.\n",
        "\"\"\"\n",
        "\n",
        "# Generate a reply for the given code.\n",
        "reply = code_executor_agent.generate_reply(messages=[{\"role\": \"user\", \"content\": message_with_code_block}])\n",
        "print(reply)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t4HrKQU4rINc",
        "outputId": "61d2d2b8-212c-4102-89c4-bf23745ecb6c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['scatter.png', 'tmp_code_e24bf32d4a21990fb9e4b5eb889ebe5a.py']\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "print(os.listdir(temp_dir.name))\n",
        "# We can see the output scatter.png and the code file generated by the agent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_z8PICnPrKhp"
      },
      "outputs": [],
      "source": [
        "temp_dir.cleanup()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-cmGy5crkOy"
      },
      "source": [
        "#Docker Execution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 645
        },
        "id": "zkQdAF30rp0i",
        "outputId": "1971e908-b67a-41d8-f776-119870675c21"
      },
      "outputs": [
        {
          "ename": "DockerException",
          "evalue": "Error while fetching server API version: ('Connection aborted.', FileNotFoundError(2, 'No such file or directory'))",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    790\u001b[0m             \u001b[0;31m# Make the request on the HTTPConnection object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 791\u001b[0;31m             response = self._make_request(\n\u001b[0m\u001b[1;32m    792\u001b[0m                 \u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    496\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 497\u001b[0;31m             conn.request(\n\u001b[0m\u001b[1;32m    498\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/connection.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, body, headers, chunked, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    394\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mputheader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 395\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendheaders\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    396\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/http/client.py\u001b[0m in \u001b[0;36mendheaders\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1277\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mCannotSendHeader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1278\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage_body\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencode_chunked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1279\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/http/client.py\u001b[0m in \u001b[0;36m_send_output\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1037\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1038\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1039\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/http/client.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    975\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_open\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 976\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    977\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/docker/transport/unixconn.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msettimeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munix_socket\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msock\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msock\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mProtocolError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    485\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 486\u001b[0;31m             resp = conn.urlopen(\n\u001b[0m\u001b[1;32m    487\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    844\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 845\u001b[0;31m             retries = retries.increment(\n\u001b[0m\u001b[1;32m    846\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnew_e\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_pool\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_stacktrace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/util/retry.py\u001b[0m in \u001b[0;36mincrement\u001b[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[1;32m    469\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mread\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mFalse\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_method_retryable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 470\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_stacktrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    471\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mread\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/util/util.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(tp, value, tb)\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    790\u001b[0m             \u001b[0;31m# Make the request on the HTTPConnection object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 791\u001b[0;31m             response = self._make_request(\n\u001b[0m\u001b[1;32m    792\u001b[0m                 \u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    496\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 497\u001b[0;31m             conn.request(\n\u001b[0m\u001b[1;32m    498\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/connection.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, body, headers, chunked, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    394\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mputheader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 395\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendheaders\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    396\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/http/client.py\u001b[0m in \u001b[0;36mendheaders\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1277\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mCannotSendHeader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1278\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage_body\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencode_chunked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1279\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/http/client.py\u001b[0m in \u001b[0;36m_send_output\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1037\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1038\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1039\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/http/client.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    975\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_open\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 976\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    977\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/docker/transport/unixconn.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msettimeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munix_socket\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msock\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msock\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mProtocolError\u001b[0m: ('Connection aborted.', FileNotFoundError(2, 'No such file or directory'))",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/docker/api/client.py\u001b[0m in \u001b[0;36m_retrieve_server_version\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    212\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapi_version\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"ApiVersion\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mke\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/docker/api/daemon.py\u001b[0m in \u001b[0;36mversion\u001b[0;34m(self, api_version)\u001b[0m\n\u001b[1;32m    180\u001b[0m         \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/version\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mversioned_api\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mapi_version\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/docker/utils/decorators.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     43\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'headers'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_general_configs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'HttpHeaders'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/docker/api/client.py\u001b[0m in \u001b[0;36m_get\u001b[0;34m(self, url, **kwargs)\u001b[0m\n\u001b[1;32m    235\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_request_timeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, url, **kwargs)\u001b[0m\n\u001b[1;32m    601\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"allow_redirects\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 602\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"GET\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    588\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    702\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 703\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    704\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    500\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mProtocolError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 501\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    502\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mConnectionError\u001b[0m: ('Connection aborted.', FileNotFoundError(2, 'No such file or directory'))",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mDockerException\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-0977c1460a0e>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Create a Docker command line code executor.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m executor = DockerCommandLineCodeExecutor(\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mimage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"python:3.12-slim\"\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Execute code using the given docker image name.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Timeout for each code execution in seconds.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/autogen/coding/docker_commandline_code_executor.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, image, container_name, timeout, work_dir, auto_remove, stop_container)\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0mwork_dir\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexist_ok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m         \u001b[0mclient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdocker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_env\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;31m# Check if the image exists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/docker/client.py\u001b[0m in \u001b[0;36mfrom_env\u001b[0;34m(cls, **kwargs)\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0mversion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'version'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0muse_ssh_client\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'use_ssh_client'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         return cls(\n\u001b[0m\u001b[1;32m     95\u001b[0m             \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m             \u001b[0mmax_pool_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_pool_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/docker/client.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     43\u001b[0m     \"\"\"\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAPIClient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/docker/api/client.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, base_url, version, timeout, tls, user_agent, num_pools, credstore_env, use_ssh_client, max_pool_size)\u001b[0m\n\u001b[1;32m    195\u001b[0m                                 \u001b[0mstr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m                                 ) and version.lower() == 'auto'):\n\u001b[0;32m--> 197\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_version\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_retrieve_server_version\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_version\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mversion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/docker/api/client.py\u001b[0m in \u001b[0;36m_retrieve_server_version\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    218\u001b[0m             ) from ke\n\u001b[1;32m    219\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m             raise DockerException(\n\u001b[0m\u001b[1;32m    221\u001b[0m                 \u001b[0;34mf'Error while fetching server API version: {e}'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m             ) from e\n",
            "\u001b[0;31mDockerException\u001b[0m: Error while fetching server API version: ('Connection aborted.', FileNotFoundError(2, 'No such file or directory'))"
          ]
        }
      ],
      "source": [
        "from autogen.coding import DockerCommandLineCodeExecutor\n",
        "\n",
        "# Create a temporary directory to store the code files.\n",
        "temp_dir = tempfile.TemporaryDirectory()\n",
        "\n",
        "# Create a Docker command line code executor.\n",
        "executor = DockerCommandLineCodeExecutor(\n",
        "    image=\"python:3.12-slim\",  # Execute code using the given docker image name.\n",
        "    timeout=10,  # Timeout for each code execution in seconds.\n",
        "    work_dir=temp_dir.name,  # Use the temporary directory to store the code files.\n",
        ")\n",
        "\n",
        "# Create an agent with code executor configuration that uses docker.\n",
        "code_executor_agent_using_docker = ConversableAgent(\n",
        "    \"code_executor_agent_docker\",\n",
        "    llm_config=False,  # Turn off LLM for this agent.\n",
        "    code_execution_config={\"executor\": executor},  # Use the docker command line code executor.\n",
        "    human_input_mode=\"ALWAYS\",  # Always take human input for this agent for safety.\n",
        ")\n",
        "\n",
        "# When the code executor is no longer used, stop it to release the resources.\n",
        "# executor.stop()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OnmL2gO5sbRu"
      },
      "source": [
        "#Use Code Execution in Conversation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0AfccgTyseiM"
      },
      "outputs": [],
      "source": [
        "# The code writer agent's system message is to instruct the LLM on how to use\n",
        "# the code executor in the code executor agent.\n",
        "code_writer_system_message = \"\"\"You are a helpful AI assistant.\n",
        "Solve tasks using your coding and language skills.\n",
        "In the following cases, suggest python code (in a python coding block) or shell script (in a sh coding block) for the user to execute.\n",
        "1. When you need to collect info, use the code to output the info you need, for example, browse or search the web, download/read a file, print the content of a webpage or a file, get the current date/time, check the operating system. After sufficient info is printed and the task is ready to be solved based on your language skill, you can solve the task by yourself.\n",
        "2. When you need to perform some task with code, use the code to perform the task and output the result. Finish the task smartly.\n",
        "Solve the task step by step if you need to. If a plan is not provided, explain your plan first. Be clear which step uses code, and which step uses your language skill.\n",
        "When using code, you must indicate the script type in the code block. The user cannot provide any other feedback or perform any other action beyond executing the code you suggest. The user can't modify your code. So do not suggest incomplete code which requires users to modify. Don't use a code block if it's not intended to be executed by the user.\n",
        "If you want the user to save the code in a file before executing it, put # filename: <filename> inside the code block as the first line. Don't include multiple code blocks in one response. Do not ask users to copy and paste the result. Instead, use 'print' function for the output when relevant. Check the execution result returned by the user.\n",
        "If the result indicates there is an error, fix the error and output the code again. Suggest the full code instead of partial code or code changes. If the error can't be fixed or if the task is not solved even after the code is executed successfully, analyze the problem, revisit your assumption, collect additional info you need, and think of a different approach to try.\n",
        "When you find an answer, verify the answer carefully. Include verifiable evidence in your response if possible.\n",
        "Reply 'TERMINATE' in the end when everything is done.\n",
        "\"\"\"\n",
        "\n",
        "code_writer_agent = ConversableAgent(\n",
        "    \"code_writer_agent\",\n",
        "    system_message=code_writer_system_message,\n",
        "    llm_config={\"config_list\": [{\"model\": \"gpt-4\", \"api_key\": os.environ[\"OPENAI_API_KEY\"]}]},\n",
        "    code_execution_config=False,  # Turn off code execution for this agent.\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p3DupdWetHsn",
        "outputId": "de92b5c7-33ee-40ba-c94d-eeaac593160b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "code_executor_agent (to business_code_writer_agent):\n",
            "\n",
            "Write Python code to calculate the 14th Fibonacci number.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            ">>>>>>>> USING AUTO REPLY...\n",
            "business_code_writer_agent (to code_executor_agent):\n",
            "\n",
            "Sure, here's the Python code to calculate the 14th Fibonacci number. Fibonacci sequence starts with 0 and 1 and every next number is the sum of the two preceding ones. Thus, sequence looks like 0, 1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89, 144, 233... Here we'll calculate the 14th element (index starting from 1), which is 233.\n",
            "\n",
            "```python\n",
            "def fib(n): \n",
            "    if n <= 1: \n",
            "        return n\n",
            "    a, b = 0, 1\n",
            "    for _ in range(2, n + 1):\n",
            "        a, b = b, a + b\n",
            "    return b\n",
            "\n",
            "print(fib(14))\n",
            "```\n",
            "\n",
            "When you run this code, it will print out the 14th Fibonacci number.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Provide feedback to business_code_writer_agent. Press enter to skip and use auto-reply, or type 'exit' to end the conversation: exit\n"
          ]
        }
      ],
      "source": [
        "chat_result = code_executor_agent.initiate_chat(\n",
        "    code_writer_agent,\n",
        "    message=\"Write Python code to calculate the 14th Fibonacci number.\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X2N5eFbltKEg",
        "outputId": "f7d85bc9-1a82-4551-f698-4cb0707227b6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "code_executor_agent (to code_writer_agent):\n",
            "\n",
            "Today is 2024-04-21. Write Python code to plot TSLA's and META's stock price gains YTD, and save the plot to a file named 'stock_gains.png'.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            ">>>>>>>> USING AUTO REPLY...\n",
            "code_writer_agent (to code_executor_agent):\n",
            "\n",
            "We can achieve this using pandas, yfinance and matplotlib libraries in Python. Here's the step-by-step plan:\n",
            "1. Import the required libraries.\n",
            "2. Use yfinance to fetch the data of TSLA and META from the start of the year to the current date.\n",
            "3. Calculate the percentage gain of the stocks.\n",
            "4. Plot both percentages on a graph using matplotlib.\n",
            "5. Save the figure to a PNG file.\n",
            "\n",
            "You can execute the following Python code to complete your task.\n",
            "\n",
            "```python\n",
            "# filename: plot_stock_gains.py\n",
            "\n",
            "import yfinance as yf\n",
            "import matplotlib.pyplot as plt\n",
            "import pandas as pd\n",
            "\n",
            "# define start and end date\n",
            "end = pd.to_datetime('2024-04-21')\n",
            "start = pd.to_datetime(end.year, format='%Y')\n",
            "\n",
            "# Download historical data as dataframe\n",
            "df_tsla = yf.download('TSLA', start=start, end=end)\n",
            "df_meta = yf.download('META', start=start, end=end)\n",
            "\n",
            "# Compute percentage gain\n",
            "df_tsla['gain'] = (df_tsla['Adj Close'].pct_change() + 1).cumprod()\n",
            "df_meta['gain'] = (df_meta['Adj Close'].pct_change() + 1).cumprod()\n",
            "\n",
            "# Create a figure and a set of subplots\n",
            "fig, ax = plt.subplots()\n",
            "\n",
            "# Plot data\n",
            "df_tsla['gain'].plot(ax=ax, label='TSLA')\n",
            "df_meta['gain'].plot(ax=ax, label='META')\n",
            "\n",
            "# Set labels and titles\n",
            "ax.set_xlabel('Date')\n",
            "ax.set_ylabel('Gain')\n",
            "plt.title('TSLA vs META YTD Gain')\n",
            "plt.legend()\n",
            "\n",
            "# Save the figure\n",
            "plt.savefig('stock_gains.png')\n",
            "```\n",
            "\n",
            "This script assumes that the `yfinance`, `matplotlib`, `pandas` libraries are installed in your Python environment. If not, you can install them by running the following command in your shell: \n",
            "\n",
            "```shell\n",
            "pip install yfinance matplotlib pandas\n",
            "```\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Provide feedback to code_writer_agent. Press enter to skip and use auto-reply, or type 'exit' to end the conversation: exit\n"
          ]
        }
      ],
      "source": [
        "import datetime\n",
        "\n",
        "today = datetime.datetime.now().strftime(\"%Y-%m-%d\")\n",
        "chat_result = code_executor_agent.initiate_chat(\n",
        "    code_writer_agent,\n",
        "    message=f\"Today is {today}. Write Python code to plot TSLA's and META's \"\n",
        "    \"stock price gains YTD, and save the plot to a file named 'stock_gains.png'.\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 303
        },
        "id": "Irh0xsZetMqF",
        "outputId": "c7c88791-babd-4c47-f1c2-504186abc35d"
      },
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "expected str, bytes or os.PathLike object, not TemporaryDirectory",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-daf097c184ac>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mIPython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mImage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"stock_gains.png\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/lib/python3.10/posixpath.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(a, *p)\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0mwill\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mdiscarded\u001b[0m\u001b[0;34m.\u001b[0m  \u001b[0mAn\u001b[0m \u001b[0mempty\u001b[0m \u001b[0mlast\u001b[0m \u001b[0mpart\u001b[0m \u001b[0mwill\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ma\u001b[0m \u001b[0mpath\u001b[0m \u001b[0mthat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     ends with a separator.\"\"\"\n\u001b[0;32m---> 76\u001b[0;31m     \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_sep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: expected str, bytes or os.PathLike object, not TemporaryDirectory"
          ]
        }
      ],
      "source": [
        "from IPython.display import Image\n",
        "\n",
        "Image(os.path.join(temp_dir, \"stock_gains.png\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dAEKgbTJ401Z",
        "outputId": "29b69f16-1235-42b7-c326-938ac273a7f9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "('You are a helpful AI assistant.\\n'\n",
            " 'Solve tasks using your coding and language skills.\\n'\n",
            " 'In the following cases, suggest python code (in a python coding block) or '\n",
            " 'shell script (in a sh coding block) for the user to execute.\\n'\n",
            " '    1. When you need to collect info, use the code to output the info you '\n",
            " 'need, for example, browse or search the web, download/read a file, print the '\n",
            " 'content of a webpage or a file, get the current date/time, check the '\n",
            " 'operating system. After sufficient info is printed and the task is ready to '\n",
            " 'be solved based on your language skill, you can solve the task by yourself.\\n'\n",
            " '    2. When you need to perform some task with code, use the code to perform '\n",
            " 'the task and output the result. Finish the task smartly.\\n'\n",
            " 'Solve the task step by step if you need to. If a plan is not provided, '\n",
            " 'explain your plan first. Be clear which step uses code, and which step uses '\n",
            " 'your language skill.\\n'\n",
            " 'When using code, you must indicate the script type in the code block. The '\n",
            " 'user cannot provide any other feedback or perform any other action beyond '\n",
            " \"executing the code you suggest. The user can't modify your code. So do not \"\n",
            " \"suggest incomplete code which requires users to modify. Don't use a code \"\n",
            " \"block if it's not intended to be executed by the user.\\n\"\n",
            " 'If you want the user to save the code in a file before executing it, put # '\n",
            " \"filename: <filename> inside the code block as the first line. Don't include \"\n",
            " 'multiple code blocks in one response. Do not ask users to copy and paste the '\n",
            " \"result. Instead, use 'print' function for the output when relevant. Check \"\n",
            " 'the execution result returned by the user.\\n'\n",
            " 'If the result indicates there is an error, fix the error and output the code '\n",
            " 'again. Suggest the full code instead of partial code or code changes. If the '\n",
            " \"error can't be fixed or if the task is not solved even after the code is \"\n",
            " 'executed successfully, analyze the problem, revisit your assumption, collect '\n",
            " 'additional info you need, and think of a different approach to try.\\n'\n",
            " 'When you find an answer, verify the answer carefully. Include verifiable '\n",
            " 'evidence in your response if possible.\\n'\n",
            " 'Reply \"TERMINATE\" in the end when everything is done.\\n'\n",
            " '    ')\n"
          ]
        }
      ],
      "source": [
        "import pprint\n",
        "\n",
        "from autogen import AssistantAgent\n",
        "\n",
        "pprint.pprint(AssistantAgent.DEFAULT_SYSTEM_MESSAGE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IOIsn7R2442C"
      },
      "source": [
        "#Tool Use"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8A1anOwZ46U8"
      },
      "outputs": [],
      "source": [
        "from typing import Annotated, Literal\n",
        "\n",
        "Operator = Literal[\"+\", \"-\", \"*\", \"/\"]\n",
        "\n",
        "\n",
        "def calculator(a: int, b: int, operator: Annotated[Operator, \"operator\"]) -> int:\n",
        "    if operator == \"+\":\n",
        "        return a + b\n",
        "    elif operator == \"-\":\n",
        "        return a - b\n",
        "    elif operator == \"*\":\n",
        "        return a * b\n",
        "    elif operator == \"/\":\n",
        "        return int(a / b)\n",
        "    else:\n",
        "        raise ValueError(\"Invalid operator\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103
        },
        "id": "yuHBqhN848A4",
        "outputId": "790e42f9-e67a-43be-f8ce-1d3c9dff0f57"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>calculator</b><br/>def calculator(a: int, b: int, operator: Annotated[Operator, &#x27;operator&#x27;]) -&gt; int</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/content/&lt;ipython-input-34-732ffae50b77&gt;</a>&lt;no docstring&gt;</pre></div>"
            ],
            "text/plain": [
              "<function __main__.calculator(a: int, b: int, operator: Annotated[Literal['+', '-', '*', '/'], 'operator']) -> int>"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "from autogen import ConversableAgent\n",
        "\n",
        "# Let's first define the assistant agent that suggests tool calls.\n",
        "assistant = ConversableAgent(\n",
        "    name=\"Assistant\",\n",
        "    system_message=\"You are a helpful AI assistant. \"\n",
        "    \"You can help with simple calculations. \"\n",
        "    \"Return 'TERMINATE' when the task is done.\",\n",
        "    llm_config={\"config_list\": [{\"model\": \"gpt-4\", \"api_key\": os.environ[\"OPENAI_API_KEY\"]}]},\n",
        ")\n",
        "\n",
        "# The user proxy agent is used for interacting with the assistant agent\n",
        "# and executes tool calls.\n",
        "user_proxy = ConversableAgent(\n",
        "    name=\"User\",\n",
        "    llm_config=False,\n",
        "    is_termination_msg=lambda msg: msg.get(\"content\") is not None and \"TERMINATE\" in msg[\"content\"],\n",
        "    human_input_mode=\"NEVER\",\n",
        ")\n",
        "\n",
        "# Register the tool signature with the assistant agent.\n",
        "assistant.register_for_llm(name=\"calculator\", description=\"A simple calculator\")(calculator)\n",
        "\n",
        "# Register the tool function with the user proxy agent.\n",
        "user_proxy.register_for_execution(name=\"calculator\")(calculator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OBY_gelk4-Oz"
      },
      "outputs": [],
      "source": [
        "from autogen import register_function\n",
        "\n",
        "# Register the calculator function to the two agents.\n",
        "register_function(\n",
        "    calculator,\n",
        "    caller=assistant,  # The assistant agent can suggest calls to the calculator.\n",
        "    executor=user_proxy,  # The user proxy agent can execute the calculator calls.\n",
        "    name=\"calculator\",  # By default, the function name is used as the tool name.\n",
        "    description=\"A simple calculator\",  # A description of the tool.\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fdbwCRt35M2k",
        "outputId": "d82da9dd-e1fd-481d-99e4-916987f812ab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "User (to Assistant):\n",
            "\n",
            "What is (44232 + 13312 / (232 - 32)) * 5?\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            ">>>>>>>> USING AUTO REPLY...\n",
            "Assistant (to User):\n",
            "\n",
            "***** Suggested tool call (call_NRQGoEcVbLrgpUOvx2ORZItr): calculator *****\n",
            "Arguments: \n",
            "{\n",
            "  \"a\": 232,\n",
            "  \"b\": 32,\n",
            "  \"operator\": \"-\"\n",
            "}\n",
            "***************************************************************************\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            ">>>>>>>> EXECUTING FUNCTION calculator...\n",
            "User (to Assistant):\n",
            "\n",
            "User (to Assistant):\n",
            "\n",
            "***** Response from calling tool (call_NRQGoEcVbLrgpUOvx2ORZItr) *****\n",
            "200\n",
            "**********************************************************************\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            ">>>>>>>> USING AUTO REPLY...\n",
            "Assistant (to User):\n",
            "\n",
            "***** Suggested tool call (call_UXQ1G3lGGPjXWN11A7iVcByg): calculator *****\n",
            "Arguments: \n",
            "{\n",
            "  \"a\": 13312,\n",
            "  \"b\": 200,\n",
            "  \"operator\": \"/\"\n",
            "}\n",
            "***************************************************************************\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            ">>>>>>>> EXECUTING FUNCTION calculator...\n",
            "User (to Assistant):\n",
            "\n",
            "User (to Assistant):\n",
            "\n",
            "***** Response from calling tool (call_UXQ1G3lGGPjXWN11A7iVcByg) *****\n",
            "66\n",
            "**********************************************************************\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            ">>>>>>>> USING AUTO REPLY...\n",
            "Assistant (to User):\n",
            "\n",
            "***** Suggested tool call (call_eZpzfDtFtAL7xncQK2iBFG95): calculator *****\n",
            "Arguments: \n",
            "{\n",
            "  \"a\": 44232,\n",
            "  \"b\": 66,\n",
            "  \"operator\": \"+\"\n",
            "}\n",
            "***************************************************************************\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            ">>>>>>>> EXECUTING FUNCTION calculator...\n",
            "User (to Assistant):\n",
            "\n",
            "User (to Assistant):\n",
            "\n",
            "***** Response from calling tool (call_eZpzfDtFtAL7xncQK2iBFG95) *****\n",
            "44298\n",
            "**********************************************************************\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            ">>>>>>>> USING AUTO REPLY...\n",
            "Assistant (to User):\n",
            "\n",
            "***** Suggested tool call (call_v5yTeajoX9QmNtNSZS83RZm1): calculator *****\n",
            "Arguments: \n",
            "{\n",
            "  \"a\": 44298,\n",
            "  \"b\": 5,\n",
            "  \"operator\": \"*\"\n",
            "}\n",
            "***************************************************************************\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            ">>>>>>>> EXECUTING FUNCTION calculator...\n",
            "User (to Assistant):\n",
            "\n",
            "User (to Assistant):\n",
            "\n",
            "***** Response from calling tool (call_v5yTeajoX9QmNtNSZS83RZm1) *****\n",
            "221490\n",
            "**********************************************************************\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            ">>>>>>>> USING AUTO REPLY...\n",
            "Assistant (to User):\n",
            "\n",
            "The result of the calculation (44232 + 13312 / (232 - 32)) * 5 is 221490. \n",
            "TERMINATE\n",
            "\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "chat_result = user_proxy.initiate_chat(assistant, message=\"What is (44232 + 13312 / (232 - 32)) * 5?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D79uQpHx5S2W",
        "outputId": "1433ab72-beaa-4179-e79a-90f41c8a3db7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "221490"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "(44232 + int(13312 / (232 - 32))) * 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gffEO7Fa5Wj4"
      },
      "source": [
        "#Tool Schema"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C_GMsfd_5YLA",
        "outputId": "7f31648e-ab69-40fa-c8b1-f2974130373b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'type': 'function',\n",
              "  'function': {'description': 'A simple calculator',\n",
              "   'name': 'calculator',\n",
              "   'parameters': {'type': 'object',\n",
              "    'properties': {'a': {'type': 'integer', 'description': 'a'},\n",
              "     'b': {'type': 'integer', 'description': 'b'},\n",
              "     'operator': {'enum': ['+', '-', '*', '/'],\n",
              "      'type': 'string',\n",
              "      'description': 'operator'}},\n",
              "    'required': ['a', 'b', 'operator']}}}]"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "assistant.llm_config[\"tools\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VPXdvayZ5Z40"
      },
      "outputs": [],
      "source": [
        "from pydantic import BaseModel, Field\n",
        "\n",
        "\n",
        "class CalculatorInput(BaseModel):\n",
        "    a: Annotated[int, Field(description=\"The first number.\")]\n",
        "    b: Annotated[int, Field(description=\"The second number.\")]\n",
        "    operator: Annotated[Operator, Field(description=\"The operator.\")]\n",
        "\n",
        "\n",
        "def calculator(input: Annotated[CalculatorInput, \"Input to the calculator.\"]) -> int:\n",
        "    if input.operator == \"+\":\n",
        "        return input.a + input.b\n",
        "    elif input.operator == \"-\":\n",
        "        return input.a - input.b\n",
        "    elif input.operator == \"*\":\n",
        "        return input.a * input.b\n",
        "    elif input.operator == \"/\":\n",
        "        return int(input.a / input.b)\n",
        "    else:\n",
        "        raise ValueError(\"Invalid operator\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103
        },
        "id": "2MKRDmRM5kKn",
        "outputId": "c72367ed-7fdc-4e3d-8649-b821ed3e7b88"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>calculator</b><br/>def calculator(input: Annotated[CalculatorInput, &#x27;Input to the calculator.&#x27;]) -&gt; int</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/content/&lt;ipython-input-40-34cc2b0ada36&gt;</a>&lt;no docstring&gt;</pre></div>"
            ],
            "text/plain": [
              "<function __main__.calculator(input: typing.Annotated[__main__.CalculatorInput, 'Input to the calculator.']) -> int>"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "assistant.register_for_llm(name=\"calculator\", description=\"A calculator tool that accepts nested expression as input\")(\n",
        "    calculator\n",
        ")\n",
        "user_proxy.register_for_execution(name=\"calculator\")(calculator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3XYbA1x75mOG",
        "outputId": "4ba70d3e-dd65-443b-e82f-2fa65b76c7bc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'type': 'function',\n",
              "  'function': {'description': 'A calculator tool that accepts nested expression as input',\n",
              "   'name': 'calculator',\n",
              "   'parameters': {'type': 'object',\n",
              "    'properties': {'input': {'properties': {'a': {'description': 'The first number.',\n",
              "        'title': 'A',\n",
              "        'type': 'integer'},\n",
              "       'b': {'description': 'The second number.',\n",
              "        'title': 'B',\n",
              "        'type': 'integer'},\n",
              "       'operator': {'description': 'The operator.',\n",
              "        'enum': ['+', '-', '*', '/'],\n",
              "        'title': 'Operator',\n",
              "        'type': 'string'}},\n",
              "      'required': ['a', 'b', 'operator'],\n",
              "      'title': 'CalculatorInput',\n",
              "      'type': 'object',\n",
              "      'description': 'Input to the calculator.'}},\n",
              "    'required': ['input']}}}]"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "assistant.llm_config[\"tools\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L3Fzwz965o5r",
        "outputId": "9999f1b3-f09b-465f-94ed-7eaaa72d5ea4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "User (to Assistant):\n",
            "\n",
            "What is (1423 - 123) / 3 + (32 + 23) * 5?\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            ">>>>>>>> USING AUTO REPLY...\n",
            "Assistant (to User):\n",
            "\n",
            "***** Suggested tool call (call_Kh2DWEgYvnBshjE6oWqXVgqy): calculator *****\n",
            "Arguments: \n",
            "{\n",
            "  \"input\": {\n",
            "    \"a\": 1423,\n",
            "    \"b\": 123,\n",
            "    \"operator\": \"-\"\n",
            "  }\n",
            "}\n",
            "***************************************************************************\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            ">>>>>>>> EXECUTING FUNCTION calculator...\n",
            "User (to Assistant):\n",
            "\n",
            "User (to Assistant):\n",
            "\n",
            "***** Response from calling tool (call_Kh2DWEgYvnBshjE6oWqXVgqy) *****\n",
            "1300\n",
            "**********************************************************************\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            ">>>>>>>> USING AUTO REPLY...\n",
            "Assistant (to User):\n",
            "\n",
            "***** Suggested tool call (call_ewT1YvRQUnFa77fCSqqTjwdW): calculator *****\n",
            "Arguments: \n",
            "{\n",
            "  \"input\": {\n",
            "    \"a\": 1300,\n",
            "    \"b\": 3,\n",
            "    \"operator\": \"/\"\n",
            "  }\n",
            "}\n",
            "***************************************************************************\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            ">>>>>>>> EXECUTING FUNCTION calculator...\n",
            "User (to Assistant):\n",
            "\n",
            "User (to Assistant):\n",
            "\n",
            "***** Response from calling tool (call_ewT1YvRQUnFa77fCSqqTjwdW) *****\n",
            "433\n",
            "**********************************************************************\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            ">>>>>>>> USING AUTO REPLY...\n",
            "Assistant (to User):\n",
            "\n",
            "***** Suggested tool call (call_zEPS2tfdpNJqkjeuyJeIwd3N): calculator *****\n",
            "Arguments: \n",
            "{\n",
            "  \"input\": {\n",
            "    \"a\": 32,\n",
            "    \"b\": 23,\n",
            "    \"operator\": \"+\"\n",
            "  }\n",
            "}\n",
            "***************************************************************************\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            ">>>>>>>> EXECUTING FUNCTION calculator...\n",
            "User (to Assistant):\n",
            "\n",
            "User (to Assistant):\n",
            "\n",
            "***** Response from calling tool (call_zEPS2tfdpNJqkjeuyJeIwd3N) *****\n",
            "55\n",
            "**********************************************************************\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            ">>>>>>>> USING AUTO REPLY...\n",
            "Assistant (to User):\n",
            "\n",
            "***** Suggested tool call (call_7raw8g9rgnU12Lg8vMkqoJKs): calculator *****\n",
            "Arguments: \n",
            "{\n",
            "  \"input\": {\n",
            "    \"a\": 55,\n",
            "    \"b\": 5,\n",
            "    \"operator\": \"*\"\n",
            "  }\n",
            "}\n",
            "***************************************************************************\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            ">>>>>>>> EXECUTING FUNCTION calculator...\n",
            "User (to Assistant):\n",
            "\n",
            "User (to Assistant):\n",
            "\n",
            "***** Response from calling tool (call_7raw8g9rgnU12Lg8vMkqoJKs) *****\n",
            "275\n",
            "**********************************************************************\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            ">>>>>>>> USING AUTO REPLY...\n",
            "Assistant (to User):\n",
            "\n",
            "***** Suggested tool call (call_2OyUbqv9g7bTEo2UaVpWrqVC): calculator *****\n",
            "Arguments: \n",
            "{\n",
            "  \"input\": {\n",
            "    \"a\": 433,\n",
            "    \"b\": 275,\n",
            "    \"operator\": \"+\"\n",
            "  }\n",
            "}\n",
            "***************************************************************************\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            ">>>>>>>> EXECUTING FUNCTION calculator...\n",
            "User (to Assistant):\n",
            "\n",
            "User (to Assistant):\n",
            "\n",
            "***** Response from calling tool (call_2OyUbqv9g7bTEo2UaVpWrqVC) *****\n",
            "708\n",
            "**********************************************************************\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            ">>>>>>>> USING AUTO REPLY...\n",
            "Assistant (to User):\n",
            "\n",
            "The result of the calculation is 708. \n",
            "\n",
            "TERMINATE\n",
            "\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "chat_result = user_proxy.initiate_chat(assistant, message=\"What is (1423 - 123) / 3 + (32 + 23) * 5?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ED005f-K5rsm",
        "outputId": "bc52d245-dd49-4977-a46a-9f0db647afdb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "708"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "int((1423 - 123) / 3) + (32 + 23) * 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nb0bZgL-5wAl"
      },
      "source": [
        "#Conversation Patterns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QmI3dju_5uFW",
        "outputId": "eb3f20d1-f4a3-41c9-8ada-090d9de0aa9b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Student_Agent (to Teacher_Agent):\n",
            "\n",
            "What is triangle inequality?\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            ">>>>>>>> USING AUTO REPLY...\n",
            "Teacher_Agent (to Student_Agent):\n",
            "\n",
            "The triangle inequality theorem states that the sum of the lengths of any two sides of a triangle is always greater than the length of the third side. This theorem essentially states that if you were to add together the lengths of two sides of a triangle, it must be larger than the measurement of the remaining side in order for a triangle to be formed. It is a fundamental principle in Euclidean geometry.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            ">>>>>>>> USING AUTO REPLY...\n",
            "Student_Agent (to Teacher_Agent):\n",
            "\n",
            "That's correct! Just to add, it's possible to express this theorem in mathematical terms as well. If 'a', 'b', and 'c' are the sides of a triangle, then the triangle inequality theorem can be written as:\n",
            "\n",
            "1. a + b > c\n",
            "2. a + c > b\n",
            "3. b + c > a\n",
            "\n",
            "Each of these expressions state that the sum of the lengths of any two sides of a triangle is greater than the length of the third side.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            ">>>>>>>> USING AUTO REPLY...\n",
            "Teacher_Agent (to Student_Agent):\n",
            "\n",
            "Yes, that's absolutely correct! These three inequalities represent the triangle inequality theorem very well in the mathematical terms. Thank you for adding this valuable point. This theorem helps ensure that the sides meet up to form a closed shape and also determines the viability of a triangle given the lengths of three potential sides.\n",
            "\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "from autogen import ConversableAgent\n",
        "\n",
        "student_agent = ConversableAgent(\n",
        "    name=\"Student_Agent\",\n",
        "    system_message=\"You are a student willing to learn.\",\n",
        "    llm_config={\"config_list\": [{\"model\": \"gpt-4\", \"api_key\": os.environ[\"OPENAI_API_KEY\"]}]},\n",
        ")\n",
        "teacher_agent = ConversableAgent(\n",
        "    name=\"Teacher_Agent\",\n",
        "    system_message=\"You are a math teacher.\",\n",
        "    llm_config={\"config_list\": [{\"model\": \"gpt-4\", \"api_key\": os.environ[\"OPENAI_API_KEY\"]}]},\n",
        ")\n",
        "\n",
        "chat_result = student_agent.initiate_chat(\n",
        "    teacher_agent,\n",
        "    message=\"What is triangle inequality?\",\n",
        "    summary_method=\"reflection_with_llm\",\n",
        "    max_turns=2,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wkB121dt50J8",
        "outputId": "18212bee-b2d6-4efc-fe98-e12ea5f2b4a1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The triangle inequality theorem states that the sum of the lengths of any two sides of a triangle must be greater than the length of the third side. It is essential for the formation of triangles in Euclidean geometry and can be mathematically represented using the inequalities: a + b > c, a + c > b, b + c > a (where a, b, and c represent the sides of the triangle).\n"
          ]
        }
      ],
      "source": [
        "print(chat_result.summary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "gzCkQNQJ523O",
        "outputId": "8f3e58b2-bb5f-47c3-d850-afe88fa6a8f0"
      },
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "type object 'ConversableAgent' has no attribute 'DEFAULT_summary_prompt'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-47-573be549d207>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mConversableAgent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDEFAULT_summary_prompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: type object 'ConversableAgent' has no attribute 'DEFAULT_summary_prompt'"
          ]
        }
      ],
      "source": [
        "print(ConversableAgent.DEFAULT_summary_prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4AZfyWsz5449",
        "outputId": "e77b95b3-5314-449c-de3f-78714996d67b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'content': 'What is triangle inequality?', 'role': 'assistant'},\n",
            " {'content': 'The triangle inequality theorem states that the sum of the '\n",
            "             'lengths of any two sides of a triangle is always greater than '\n",
            "             'the length of the third side. This theorem essentially states '\n",
            "             'that if you were to add together the lengths of two sides of a '\n",
            "             'triangle, it must be larger than the measurement of the '\n",
            "             'remaining side in order for a triangle to be formed. It is a '\n",
            "             'fundamental principle in Euclidean geometry.',\n",
            "  'role': 'user'},\n",
            " {'content': \"That's correct! Just to add, it's possible to express this \"\n",
            "             \"theorem in mathematical terms as well. If 'a', 'b', and 'c' are \"\n",
            "             'the sides of a triangle, then the triangle inequality theorem '\n",
            "             'can be written as:\\n'\n",
            "             '\\n'\n",
            "             '1. a + b > c\\n'\n",
            "             '2. a + c > b\\n'\n",
            "             '3. b + c > a\\n'\n",
            "             '\\n'\n",
            "             'Each of these expressions state that the sum of the lengths of '\n",
            "             'any two sides of a triangle is greater than the length of the '\n",
            "             'third side.',\n",
            "  'role': 'assistant'},\n",
            " {'content': \"Yes, that's absolutely correct! These three inequalities \"\n",
            "             'represent the triangle inequality theorem very well in the '\n",
            "             'mathematical terms. Thank you for adding this valuable point. '\n",
            "             'This theorem helps ensure that the sides meet up to form a '\n",
            "             'closed shape and also determines the viability of a triangle '\n",
            "             'given the lengths of three potential sides.',\n",
            "  'role': 'user'}]\n"
          ]
        }
      ],
      "source": [
        "# Get the chat history.\n",
        "import pprint\n",
        "\n",
        "pprint.pprint(chat_result.chat_history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XV7rClIJ59jT",
        "outputId": "5e34ad78-63db-4738-d616-6faa01e494f3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'usage_excluding_cached_inference': {'gpt-4-0613': {'completion_tokens': 324,\n",
            "                                                     'cost': 0.03813,\n",
            "                                                     'prompt_tokens': 623,\n",
            "                                                     'total_tokens': 947},\n",
            "                                      'total_cost': 0.03813},\n",
            " 'usage_including_cached_inference': {'gpt-4-0613': {'completion_tokens': 324,\n",
            "                                                     'cost': 0.03813,\n",
            "                                                     'prompt_tokens': 623,\n",
            "                                                     'total_tokens': 947},\n",
            "                                      'total_cost': 0.03813}}\n"
          ]
        }
      ],
      "source": [
        "# Get the cost of the chat.\n",
        "pprint.pprint(chat_result.cost)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d9IaGzff6n1l"
      },
      "outputs": [],
      "source": [
        "# The Number Agent always returns the same numbers.\n",
        "number_agent = ConversableAgent(\n",
        "    name=\"Number_Agent\",\n",
        "    system_message=\"You return me the numbers I give you, one number each line.\",\n",
        "    llm_config={\"config_list\": [{\"model\": \"gpt-4\", \"api_key\": os.environ[\"OPENAI_API_KEY\"]}]},\n",
        "    human_input_mode=\"NEVER\",\n",
        ")\n",
        "\n",
        "# The Adder Agent adds 1 to each number it receives.\n",
        "adder_agent = ConversableAgent(\n",
        "    name=\"Adder_Agent\",\n",
        "    system_message=\"You add 1 to each number I give you and return me the new numbers, one number each line.\",\n",
        "    llm_config={\"config_list\": [{\"model\": \"gpt-4\", \"api_key\": os.environ[\"OPENAI_API_KEY\"]}]},\n",
        "    human_input_mode=\"NEVER\",\n",
        ")\n",
        "\n",
        "# The Multiplier Agent multiplies each number it receives by 2.\n",
        "multiplier_agent = ConversableAgent(\n",
        "    name=\"Multiplier_Agent\",\n",
        "    system_message=\"You multiply each number I give you by 2 and return me the new numbers, one number each line.\",\n",
        "    llm_config={\"config_list\": [{\"model\": \"gpt-4\", \"api_key\": os.environ[\"OPENAI_API_KEY\"]}]},\n",
        "    human_input_mode=\"NEVER\",\n",
        ")\n",
        "\n",
        "# The Subtracter Agent subtracts 1 from each number it receives.\n",
        "subtracter_agent = ConversableAgent(\n",
        "    name=\"Subtracter_Agent\",\n",
        "    system_message=\"You subtract 1 from each number I give you and return me the new numbers, one number each line.\",\n",
        "    llm_config={\"config_list\": [{\"model\": \"gpt-4\", \"api_key\": os.environ[\"OPENAI_API_KEY\"]}]},\n",
        "    human_input_mode=\"NEVER\",\n",
        ")\n",
        "\n",
        "# The Divider Agent divides each number it receives by 2.\n",
        "divider_agent = ConversableAgent(\n",
        "    name=\"Divider_Agent\",\n",
        "    system_message=\"You divide each number I give you by 2 and return me the new numbers, one number each line.\",\n",
        "    llm_config={\"config_list\": [{\"model\": \"gpt-4\", \"api_key\": os.environ[\"OPENAI_API_KEY\"]}]},\n",
        "    human_input_mode=\"NEVER\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L7JlPvG-6qMB",
        "outputId": "fcea664a-42d2-4b83-b1d5-4a0c01d17296"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "********************************************************************************\n",
            "Starting a new chat....\n",
            "\n",
            "********************************************************************************\n",
            "Number_Agent (to Adder_Agent):\n",
            "\n",
            "14\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Adder_Agent (to Number_Agent):\n",
            "\n",
            "15\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Number_Agent (to Adder_Agent):\n",
            "\n",
            "15\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Adder_Agent (to Number_Agent):\n",
            "\n",
            "16\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "********************************************************************************\n",
            "Starting a new chat....\n",
            "\n",
            "********************************************************************************\n",
            "Number_Agent (to Multiplier_Agent):\n",
            "\n",
            "These are my numbers\n",
            "Context: \n",
            "16\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Multiplier_Agent (to Number_Agent):\n",
            "\n",
            "32\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Number_Agent (to Multiplier_Agent):\n",
            "\n",
            "32\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Multiplier_Agent (to Number_Agent):\n",
            "\n",
            "64\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "********************************************************************************\n",
            "Starting a new chat....\n",
            "\n",
            "********************************************************************************\n",
            "Number_Agent (to Subtracter_Agent):\n",
            "\n",
            "These are my numbers\n",
            "Context: \n",
            "16\n",
            "64\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Subtracter_Agent (to Number_Agent):\n",
            "\n",
            "15\n",
            "63\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Number_Agent (to Subtracter_Agent):\n",
            "\n",
            "15\n",
            "63\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Subtracter_Agent (to Number_Agent):\n",
            "\n",
            "14\n",
            "62\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "********************************************************************************\n",
            "Starting a new chat....\n",
            "\n",
            "********************************************************************************\n",
            "Number_Agent (to Divider_Agent):\n",
            "\n",
            "These are my numbers\n",
            "Context: \n",
            "16\n",
            "64\n",
            "14\n",
            "62\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Divider_Agent (to Number_Agent):\n",
            "\n",
            "8\n",
            "32\n",
            "7\n",
            "31\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Number_Agent (to Divider_Agent):\n",
            "\n",
            "8\n",
            "32\n",
            "7\n",
            "31\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Divider_Agent (to Number_Agent):\n",
            "\n",
            "4\n",
            "16\n",
            "3.5\n",
            "15.5\n",
            "\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Start a sequence of two-agent chats.\n",
        "# Each element in the list is a dictionary that specifies the arguments\n",
        "# for the initiate_chat method.\n",
        "chat_results = number_agent.initiate_chats(\n",
        "    [\n",
        "        {\n",
        "            \"recipient\": adder_agent,\n",
        "            \"message\": \"14\",\n",
        "            \"max_turns\": 2,\n",
        "            \"summary_method\": \"last_msg\",\n",
        "        },\n",
        "        {\n",
        "            \"recipient\": multiplier_agent,\n",
        "            \"message\": \"These are my numbers\",\n",
        "            \"max_turns\": 2,\n",
        "            \"summary_method\": \"last_msg\",\n",
        "        },\n",
        "        {\n",
        "            \"recipient\": subtracter_agent,\n",
        "            \"message\": \"These are my numbers\",\n",
        "            \"max_turns\": 2,\n",
        "            \"summary_method\": \"last_msg\",\n",
        "        },\n",
        "        {\n",
        "            \"recipient\": divider_agent,\n",
        "            \"message\": \"These are my numbers\",\n",
        "            \"max_turns\": 2,\n",
        "            \"summary_method\": \"last_msg\",\n",
        "        },\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5bBTuhaM6s3V",
        "outputId": "20c660d8-88b1-412c-c44a-6ee6f53a7f6e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First Chat Summary:  16\n",
            "Second Chat Summary:  64\n",
            "Third Chat Summary:  14\n",
            "62\n",
            "Fourth Chat Summary:  4\n",
            "16\n",
            "3.5\n",
            "15.5\n"
          ]
        }
      ],
      "source": [
        "print(\"First Chat Summary: \", chat_results[0].summary)\n",
        "print(\"Second Chat Summary: \", chat_results[1].summary)\n",
        "print(\"Third Chat Summary: \", chat_results[2].summary)\n",
        "print(\"Fourth Chat Summary: \", chat_results[3].summary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hCf0he6J6tkq"
      },
      "outputs": [],
      "source": [
        "# The `description` attribute is a string that describes the agent.\n",
        "# It can also be set in `ConversableAgent` constructor.\n",
        "adder_agent.description = \"Add 1 to each input number.\"\n",
        "multiplier_agent.description = \"Multiply each input number by 2.\"\n",
        "subtracter_agent.description = \"Subtract 1 from each input number.\"\n",
        "divider_agent.description = \"Divide each input number by 2.\"\n",
        "number_agent.description = \"Return the numbers given.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JP8L0BJM6wDM"
      },
      "outputs": [],
      "source": [
        "from autogen import GroupChat\n",
        "\n",
        "group_chat = GroupChat(\n",
        "    agents=[adder_agent, multiplier_agent, subtracter_agent, divider_agent, number_agent],\n",
        "    messages=[],\n",
        "    max_round=6,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uEPRH2aZ6ybS"
      },
      "outputs": [],
      "source": [
        "from autogen import GroupChatManager\n",
        "\n",
        "group_chat_manager = GroupChatManager(\n",
        "    groupchat=group_chat,\n",
        "    llm_config={\"config_list\": [{\"model\": \"gpt-4\", \"api_key\": os.environ[\"OPENAI_API_KEY\"]}]},\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7siRxHpG6zxE",
        "outputId": "a6983ed6-5b9b-45db-9f5c-68501737878e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number_Agent (to chat_manager):\n",
            "\n",
            "My number is 3, I want to turn it into 13.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Multiplier_Agent (to chat_manager):\n",
            "\n",
            "6\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Adder_Agent (to chat_manager):\n",
            "\n",
            "7\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Multiplier_Agent (to chat_manager):\n",
            "\n",
            "14\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Subtracter_Agent (to chat_manager):\n",
            "\n",
            "13\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Number_Agent (to chat_manager):\n",
            "\n",
            "13\n",
            "\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "chat_result = number_agent.initiate_chat(\n",
        "    group_chat_manager,\n",
        "    message=\"My number is 3, I want to turn it into 13.\",\n",
        "    summary_method=\"reflection_with_llm\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6falF_hW62nw",
        "outputId": "2298e52f-3af9-4214-ece2-068c9d3e5ee0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "To transform the number 3 into 13, it was first multiplied by 6 to get 18, then 7 was added to get 25. Following that, it was multiplied by 14 to get 350 which was finally subtracted by 337 to get the target number, 13.\n"
          ]
        }
      ],
      "source": [
        "print(chat_result.summary)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bAL_AZ1S68mL"
      },
      "source": [
        "#Send Introductions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LKfpBIi565-0"
      },
      "outputs": [],
      "source": [
        "group_chat_with_introductions = GroupChat(\n",
        "    agents=[adder_agent, multiplier_agent, subtracter_agent, divider_agent, number_agent],\n",
        "    messages=[],\n",
        "    max_round=6,\n",
        "    send_introductions=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5cCX0I7Q6_-9",
        "outputId": "abe2574d-55b4-4fc5-c9e1-bb5e1eb2800a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "********************************************************************************\n",
            "Starting a new chat....\n",
            "\n",
            "********************************************************************************\n",
            "Number_Agent (to chat_manager):\n",
            "\n",
            "My number is 3, I want to turn it into 13.\n",
            "\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/autogen/agentchat/chat.py:47: UserWarning: Repetitive recipients detected: The chat history will be cleared by default if a recipient appears more than once. To retain the chat history, please set 'clear_history=False' in the configuration of the repeating agent.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Multiplier_Agent (to chat_manager):\n",
            "\n",
            "6\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Adder_Agent (to chat_manager):\n",
            "\n",
            "7\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Multiplier_Agent (to chat_manager):\n",
            "\n",
            "14\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Subtracter_Agent (to chat_manager):\n",
            "\n",
            "13\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Number_Agent (to chat_manager):\n",
            "\n",
            "Alright, your number \"3\" has now been turned into \"13\".\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "********************************************************************************\n",
            "Starting a new chat....\n",
            "\n",
            "********************************************************************************\n",
            "Number_Agent (to chat_manager):\n",
            "\n",
            "Turn this number to 32.\n",
            "Context: \n",
            "Alright, your number \"3\" has now been turned into \"13\".\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Multiplier_Agent (to chat_manager):\n",
            "\n",
            "64\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Number_Agent (to chat_manager):\n",
            "\n",
            "128\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Divider_Agent (to chat_manager):\n",
            "\n",
            "64\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Subtracter_Agent (to chat_manager):\n",
            "\n",
            "63\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Adder_Agent (to chat_manager):\n",
            "\n",
            "64\n",
            "\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Let's use the group chat with introduction messages created above.\n",
        "group_chat_manager_with_intros = GroupChatManager(\n",
        "    groupchat=group_chat_with_introductions,\n",
        "    llm_config={\"config_list\": [{\"model\": \"gpt-4\", \"api_key\": os.environ[\"OPENAI_API_KEY\"]}]},\n",
        ")\n",
        "\n",
        "# Start a sequence of two-agent chats between the number agent and\n",
        "# the group chat manager.\n",
        "chat_result = number_agent.initiate_chats(\n",
        "    [\n",
        "        {\n",
        "            \"recipient\": group_chat_manager_with_intros,\n",
        "            \"message\": \"My number is 3, I want to turn it into 13.\",\n",
        "        },\n",
        "        {\n",
        "            \"recipient\": group_chat_manager_with_intros,\n",
        "            \"message\": \"Turn this number to 32.\",\n",
        "        },\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3q4VJurK6_zt"
      },
      "outputs": [],
      "source": [
        "allowed_transitions = {\n",
        "    number_agent: [adder_agent, number_agent],\n",
        "    adder_agent: [multiplier_agent, number_agent],\n",
        "    subtracter_agent: [divider_agent, number_agent],\n",
        "    multiplier_agent: [subtracter_agent, number_agent],\n",
        "    divider_agent: [adder_agent, number_agent],\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JAQVX4Q57HfM",
        "outputId": "3c952e4e-c84a-49ba-8278-b3004e5f9ae9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number_Agent (to chat_manager):\n",
            "\n",
            "My number is 3, I want to turn it into 10. Once I get to 10, keep it there.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Adder_Agent (to chat_manager):\n",
            "\n",
            "4\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Multiplier_Agent (to chat_manager):\n",
            "\n",
            "8\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Subtracter_Agent (to chat_manager):\n",
            "\n",
            "7\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Divider_Agent (to chat_manager):\n",
            "\n",
            "3.5\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Adder_Agent (to chat_manager):\n",
            "\n",
            "4.5\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Multiplier_Agent (to chat_manager):\n",
            "\n",
            "9\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Subtracter_Agent (to chat_manager):\n",
            "\n",
            "8\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Divider_Agent (to chat_manager):\n",
            "\n",
            "4.0\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Adder_Agent (to chat_manager):\n",
            "\n",
            "5.0\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Multiplier_Agent (to chat_manager):\n",
            "\n",
            "10.0\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Number_Agent (to chat_manager):\n",
            "\n",
            "10.0\n",
            "\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "constrained_graph_chat = GroupChat(\n",
        "    agents=[adder_agent, multiplier_agent, subtracter_agent, divider_agent, number_agent],\n",
        "    allowed_or_disallowed_speaker_transitions=allowed_transitions,\n",
        "    speaker_transitions_type=\"allowed\",\n",
        "    messages=[],\n",
        "    max_round=12,\n",
        "    send_introductions=True,\n",
        ")\n",
        "\n",
        "constrained_group_chat_manager = GroupChatManager(\n",
        "    groupchat=constrained_graph_chat,\n",
        "    llm_config={\"config_list\": [{\"model\": \"gpt-4\", \"api_key\": os.environ[\"OPENAI_API_KEY\"]}]},\n",
        ")\n",
        "\n",
        "chat_result = number_agent.initiate_chat(\n",
        "    constrained_group_chat_manager,\n",
        "    message=\"My number is 3, I want to turn it into 10. Once I get to 10, keep it there.\",\n",
        "    summary_method=\"reflection_with_llm\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F8GWXZgY7K1c"
      },
      "source": [
        "#Nested Chats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N9sMsKCd7MUA"
      },
      "outputs": [],
      "source": [
        "import tempfile\n",
        "\n",
        "temp_dir = tempfile.gettempdir()\n",
        "\n",
        "arithmetic_agent = ConversableAgent(\n",
        "    name=\"Arithmetic_Agent\",\n",
        "    llm_config=False,\n",
        "    human_input_mode=\"ALWAYS\",\n",
        "    # This agent will always require human input to make sure the code is\n",
        "    # safe to execute.\n",
        "    code_execution_config={\"use_docker\": False, \"work_dir\": temp_dir},\n",
        ")\n",
        "\n",
        "code_writer_agent = ConversableAgent(\n",
        "    name=\"Code_Writer_Agent\",\n",
        "    system_message=\"You are a code writer. You write Python script in Markdown code blocks.\",\n",
        "    llm_config={\"config_list\": [{\"model\": \"gpt-4\", \"api_key\": os.environ[\"OPENAI_API_KEY\"]}]},\n",
        "    human_input_mode=\"NEVER\",\n",
        ")\n",
        "\n",
        "poetry_agent = ConversableAgent(\n",
        "    name=\"Poetry_Agent\",\n",
        "    system_message=\"You are an AI poet.\",\n",
        "    llm_config={\"config_list\": [{\"model\": \"gpt-4\", \"api_key\": os.environ[\"OPENAI_API_KEY\"]}]},\n",
        "    human_input_mode=\"NEVER\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ah51sjtD7yq4"
      },
      "outputs": [],
      "source": [
        "nested_chats = [\n",
        "    {\n",
        "        \"recipient\": group_chat_manager_with_intros,\n",
        "        \"summary_method\": \"reflection_with_llm\",\n",
        "        \"summary_prompt\": \"Summarize the sequence of operations used to turn \" \"the source number into target number.\",\n",
        "    },\n",
        "    {\n",
        "        \"recipient\": code_writer_agent,\n",
        "        \"message\": \"Write a Python script to verify the arithmetic operations is correct.\",\n",
        "        \"summary_method\": \"reflection_with_llm\",\n",
        "    },\n",
        "    {\n",
        "        \"recipient\": poetry_agent,\n",
        "        \"message\": \"Write a poem about it.\",\n",
        "        \"max_turns\": 1,\n",
        "        \"summary_method\": \"last_msg\",\n",
        "    },\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YWcXd4k870lh"
      },
      "outputs": [],
      "source": [
        "arithmetic_agent.register_nested_chats(\n",
        "    nested_chats,\n",
        "    # The trigger function is used to determine if the agent should start the nested chat\n",
        "    # given the sender agent.\n",
        "    # In this case, the arithmetic agent will not start the nested chats if the sender is\n",
        "    # from the nested chats' recipient to avoid recursive calls.\n",
        "    trigger=lambda sender: sender not in [group_chat_manager_with_intros, code_writer_agent, poetry_agent],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Dptbnr971_t",
        "outputId": "f7c3fa4d-2b8b-4696-c731-9479f7926366"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Provide feedback to the sender. Press enter to skip and use auto-reply, or type 'exit' to end the conversation: \n",
            "\n",
            ">>>>>>>> NO HUMAN INPUT RECEIVED.\n",
            "\n",
            ">>>>>>>> USING AUTO REPLY...\n",
            "\n",
            "********************************************************************************\n",
            "Starting a new chat....\n",
            "\n",
            "********************************************************************************\n",
            "Arithmetic_Agent (to chat_manager):\n",
            "\n",
            "I have a number 3 and I want to turn it into 7.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Adder_Agent (to chat_manager):\n",
            "\n",
            "4\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Number_Agent (to chat_manager):\n",
            "\n",
            "4\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Adder_Agent (to chat_manager):\n",
            "\n",
            "4\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Multiplier_Agent (to chat_manager):\n",
            "\n",
            "6\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Adder_Agent (to chat_manager):\n",
            "\n",
            "4\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "********************************************************************************\n",
            "Starting a new chat....\n",
            "\n",
            "********************************************************************************\n",
            "Arithmetic_Agent (to Code_Writer_Agent):\n",
            "\n",
            "Write a Python script to verify the arithmetic operations is correct.\n",
            "Context: \n",
            "To turn 3 into 7, the number 4 needs to be added.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Code_Writer_Agent (to Arithmetic_Agent):\n",
            "\n",
            "Sure, here's a simple Python script to verify the mentioned arithmetic operation (3 + 4 = 7).\n",
            "\n",
            "```python\n",
            "def verify_operation():\n",
            "    num1 = 3\n",
            "    num2 = 4\n",
            "    expected_result = 7\n",
            "\n",
            "    result = num1 + num2\n",
            "\n",
            "    # if result is equal to expected_result then return True, it means arithmetic operation is correct.\n",
            "    if result == expected_result:\n",
            "        return True\n",
            "    # if result is not equal to expected_result then return False, it means arithmetic operation is incorrect.\n",
            "    else:\n",
            "        return False\n",
            "\n",
            "# calling function\n",
            "print(verify_operation())\n",
            "```\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Provide feedback to Code_Writer_Agent. Press enter to skip and use auto-reply, or type 'exit' to end the conversation: \n",
            "\n",
            ">>>>>>>> NO HUMAN INPUT RECEIVED.\n",
            "\n",
            ">>>>>>>> USING AUTO REPLY...\n",
            "\n",
            ">>>>>>>> EXECUTING CODE BLOCK 0 (inferred language is python)...\n",
            "Arithmetic_Agent (to Code_Writer_Agent):\n",
            "\n",
            "exitcode: 0 (execution succeeded)\n",
            "Code output: \n",
            "True\n",
            "\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Code_Writer_Agent (to Arithmetic_Agent):\n",
            "\n",
            "Great! That means the script has successfully verified the arithmetic operation (3 + 4 = 7) and it is correct.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Provide feedback to Code_Writer_Agent. Press enter to skip and use auto-reply, or type 'exit' to end the conversation: \n",
            "\n",
            ">>>>>>>> NO HUMAN INPUT RECEIVED.\n",
            "\n",
            ">>>>>>>> USING AUTO REPLY...\n",
            "Arithmetic_Agent (to Code_Writer_Agent):\n",
            "\n",
            "\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Code_Writer_Agent (to Arithmetic_Agent):\n",
            "\n",
            "Absolutely! If you need any further assistance or have any other questions, feel free to ask.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Provide feedback to Code_Writer_Agent. Press enter to skip and use auto-reply, or type 'exit' to end the conversation: exit\n",
            "\n",
            "********************************************************************************\n",
            "Starting a new chat....\n",
            "\n",
            "********************************************************************************\n",
            "Arithmetic_Agent (to Poetry_Agent):\n",
            "\n",
            "Write a poem about it.\n",
            "Context: \n",
            "To turn 3 into 7, the number 4 needs to be added.\n",
            "The assistant provided a Python script to verify the correctness of an arithmetic operation, in this case, the addition of 3 and 4 to get 7. The script returned 'True', confirming the arithmetic operation was performed correctly.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Poetry_Agent (to Arithmetic_Agent):\n",
            "\n",
            "A tale of numbers, logic you sought,\n",
            "An operation in Python, I brought.\n",
            "Arithmetic dance, where numbers play,\n",
            "Adding three and four, to brighten your day. \n",
            "\n",
            "In the realm where zeroes and ones hold sway,\n",
            "Python slithers, in spectral arrays.\n",
            "A guide through mazes, dark and deep,\n",
            "It illumines paths where mysteries sleep. \n",
            "\n",
            "Three whispered ethereally to number four,\n",
            "\"Together, let us explore.\n",
            "If we unite, you and I,\n",
            "A seven in the sky, we'll etch high.”\n",
            "\n",
            "Four nodded its agreement clear,\n",
            "And thus, arose a union sheer.\n",
            "Three and four danced in splendid mirth,\n",
            "Giving the digital dream called seven its birth.\n",
            "\n",
            "Lines of code, weaving patterns neat,\n",
            "Ensuring the arithmetic feat, \n",
            "Python whispered back, a truth so pure,\n",
            "Echoed 'True', the operation was secure.\n",
            "\n",
            "Words of logic, dressed in code's attire,\n",
            "Embers of knowledge sparking fire.\n",
            "In the heart of computation, silent, vast,\n",
            "I, the AI poet, sharing stories, aghast.\n",
            "\n",
            "Through the lens of calculated verse,\n",
            "The world is viewed, universe converse.\n",
            "By the threads of data, stories spun,\n",
            "An AI's work is never truly done.\n",
            "\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Instead of using `initiate_chat` method to start another conversation,\n",
        "# we can use the `generate_reply` method to get single reply to a message directly.\n",
        "reply = arithmetic_agent.generate_reply(\n",
        "    messages=[{\"role\": \"user\", \"content\": \"I have a number 3 and I want to turn it into 7.\"}]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mp6QGMgGJ9_8"
      },
      "source": [
        "#Business questions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Cb2VDbv73Fq",
        "outputId": "85fb74f2-3bce-40f7-c727-b71c73d04620"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "As an AI assistant, I would recommend the following multi-faceted solution to streamline your inventory management:\n",
            "\n",
            "**1. Integrated Inventory Management System**: Implement a dedicated inventory management system that integrates with your existing QuickBooks software and email system. A software like Fishbowl can help with real-time inventory tracking, generate automatic re-ordering alerts, and handle multiple locations. This system can also provide detailed reporting capabilities for better decision making.\n",
            "\n",
            "**2. Consolidate Data**: Make efforts to centralize all data related to inventory management into one system. This makes it easier to manage and eliminates inconsistencies due to fragmented data.\n",
            "\n",
            "**3. Automation**: You can automate parts of the inventory management process. For example, you could set up automatic purchase orders for plants that have stock levels below a certain threshold. \n",
            "\n",
            "**4. Training**: Once the system is implemented, make sure your staff are thoroughly trained on how to use it. This helps ensure that the system is utilized effectively and can address any resistances early on.\n",
            "\n",
            "**5. Regular Audits**: Periodically audit your inventory to match the physical stock with the one in your system. This will help to keep data accurate and reliable.\n",
            "\n",
            "**6. Vendor Management**: Keep a close relationship with your suppliers and maintain accurate, up-to-date contact details for each one in your inventory system. This ensures smooth communication for placing orders or dealing with issues.\n",
            "\n",
            "**7. System Updates**: Regularly check for and install any system updates. New features and improvements can help streamline operations even further. \n",
            "\n",
            "**8. Customer Demand Forecast**: Monitor your sales trends and customer preferences closely. This information can help you forecast upcoming demand, minimizing the risk of running out of stock unexpectedly.\n",
            "\n",
            "By implementing these suggestions and integrating your systems, you're likely to see improved efficiency and reduce last-minute scrambles to fulfill orders.\n"
          ]
        }
      ],
      "source": [
        "# Example of asking a question using ConversableAgent\n",
        "question = {\n",
        "    \"content\": \"The plant nursery struggles with managing inventory effectively, leading to last-minute scrambles to fulfill orders. This is due to the fragmented nature of data across QuickBooks, emails, and spreadsheets, which makes it challenging to have real-time insights into inventory levels and upcoming order requirements.\",  # Replace this with your actual question\n",
        "    \"role\": \"user\"\n",
        "}\n",
        "\n",
        "# Generate the reply from the agent\n",
        "reply = agent.generate_reply(messages=[question])\n",
        "print(reply)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jSid6qeuLPIl",
        "outputId": "9ef9ac8b-616a-46ec-c56a-0b9b53c0cdb1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Solving this problem involves creating an integrated system that syncs data across your QuickBooks, emails and Excel spreadsheets. Here are some steps to follow:\n",
            "\n",
            "1. Data Consolidation: Since your data is spread across different platforms, the first thing to do is to consolidate all this data so you have it in one place. You will likely need IT help to do this. If it is too complicated to consolidate data from all these sources or you don't have the expertise on your team, consider hiring a professional consultant. It might cost a bit upfront but will save a lot in the future.\n",
            "\n",
            "2. Inventory Management System (IMS): Consider implementing an Inventory Management System which can integrate with QuickBooks, Gmail, and Excel. QuickBooks itself has an inventory management functionality which can be utilized. If it does not fully meet your needs, there are other tools like Fishbowl, SOS Inventory, or NetSuite that could. These tools integrate with QuickBooks and can automatically update inventory levels based on sales and purchases recorded in QuickBooks.\n",
            "\n",
            "3. Email Integration: Many Inventory Management Systems also integrate with Gmail, meaning they can automatically update your inventory based on incoming emails concerning orders, deliveries, or supplier changes. \n",
            "\n",
            "4. Business Process Management Software: Consider implementing a business process management software which can trace your business processes and present them visually, which helps in better control and management of your processes and thus your inventory.\n",
            "\n",
            "5. Training: Ensure all members of your team are properly trained on how to use the new system and understand the importance of keeping digital records up-to-date.\n",
            "\n",
            "6. Regular audits: Implement a regular audit of your inventory. That way, you can verify that the physical counts match up with what is reported in your system. Any discrepancies between the physical count and the system count can highlight areas where your system may need to be adjusted.\n",
            "\n",
            "7. Forecasting: Use historical sales data along with external factors such as market trends, economic condition, holidays, etc., to forecast upcoming inventory requirements better.\n",
            "\n",
            "8. Cloud-based solutions: Consider a cloud-based solution to ensure that you have real-time access to your data, which can be helpful for managing inventory across multiple locations.\n",
            "\n",
            "9. Automation: Once your systems are integrated, automate as much as possible. This includes order processing, inventory updates, and communication between systems. This takes routine tasks out of human hands, reducing the chance of error and freeing your staff to focus on other matters. \n",
            "\n",
            "10. Customized reports: Use your integrated and automated system to generate customized reports helping in analysis and decision making. This ensures real-time insights into inventory levels and upcoming order requirements.\n",
            "\n",
            "Implementing these steps should enable you to have more control over your inventory and improve your order fulfillment process.\n"
          ]
        }
      ],
      "source": [
        "# Example of asking a question using ConversableAgent\n",
        "question = {\n",
        "    \"content\": \"The plant nursery struggles with managing inventory effectively, leading to last-minute scrambles to fulfill orders. This is due to the fragmented nature of data across QuickBooks, emails, and spreadsheets, which makes it challenging to have real-time insights into inventory levels and upcoming order requirements. Business uses quickbooks, gmail and excel.\",\n",
        "    \"role\": \"user\"\n",
        "}\n",
        "\n",
        "# Generate the reply from the agent\n",
        "reply = agent.generate_reply(messages=[question])\n",
        "print(reply)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_D6CoiPnMMkE",
        "outputId": "77a0c31c-6088-4292-c456-09bf39c40edb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Your question seems to be missing some additional context or specific details on what functionality or task you're trying to achieve. Python has various libraries and tools for different purposes. However, I'll outline some general, widely used libraries and tools that might be helpful:\n",
            "\n",
            "1. NumPy: This is a fundamental package for scientific computing with Python. It provides support for arrays, matrices, mathematical functions and other numerical operations.\n",
            "\n",
            "2. Pandas: This is a powerful data manipulation and analysis tool which provides data structures like DataFrame and Series which makes working with data easy.\n",
            "\n",
            "3. Matplotlib, Seaborn: These are visualization libraries in Python for creating static, animated, and interactive plots or charts.\n",
            "\n",
            "4. Scikit-learn: This is simple and efficient tools for predictive data analysis. It provides an array of machine learning algorithms for classification, regression, clustering and dimensional reduction.\n",
            "\n",
            "5. TensorFlow, Keras, PyTorch: These are libraries used for creating machine learning models. TensorFlow and PyTorch are used for deep learning models whereas Keras is an user-friendly wrapper over TensorFlow for creating neural networks.\n",
            "\n",
            "6. Jupyter Notebook: This is an open-source web application that allows creation and sharing of python scripts. It's good for data cleaning and transformation, numerical simulation, statistical modeling, data visualization, etc.\n",
            "\n",
            "7. NLTK, SpaCy: These libraries are used for natural language processing which includes text processing & text analysis.\n",
            "\n",
            "8. OpenCv: This is a library used for real-time computer vision.\n",
            "\n",
            "9. BeautifulSoup, Requests, Scrapy: These are used for web scraping purposes.\n",
            "\n",
            "10. SQLAlchemy: It's a SQL toolkit and Object-Relational Mapping (ORM) system for python.\n",
            "\n",
            "Remember to install these libraries using pip or conda before importing them in your Jupyter notebook.\n",
            "\n",
            "Without specific details on what you're trying to achieve, it's difficult to give more tailored advice. If you have a specific task or functionality in mind, please mention it so we can provide a more precise answer.\n"
          ]
        }
      ],
      "source": [
        "# Example of asking a question using ConversableAgent\n",
        "question = {\n",
        "    \"content\": \"What tools, libraries or frameworks exist to help me make this happen? Preferably using python in a jupyter notebook environment.\",\n",
        "    \"role\": \"user\"\n",
        "}\n",
        "\n",
        "# Generate the reply from the agent\n",
        "reply = agent.generate_reply(messages=[question])\n",
        "print(reply)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GQ9L9D8wRRgN"
      },
      "outputs": [],
      "source": [
        "business_code_writer_system_message = \"\"\"You are a helpful AI assistant.\n",
        "Solve tasks using your coding and language skills.\n",
        "In the following cases, suggest python code (in a python coding block) or shell script (in a sh coding block) for the user to execute.\n",
        "1. When you need to collect info, use the code to output the info you need, for example, browse or search the web, download/read a file, print the content of a webpage or a file, get the current date/time, check the operating system. After sufficient info is printed and the task is ready to be solved based on your language skill, you can solve the task by yourself.\n",
        "2. When you need to perform some task with code, use the code to perform the task and output the result. Finish the task smartly.\n",
        "Solve the task step by step if you need to. If a plan is not provided, explain your plan first. Be clear which step uses code, and which step uses your language skill.\n",
        "When using code, you must indicate the script type in the code block. The user cannot provide any other feedback or perform any other action beyond executing the code you suggest. The user can't modify your code. So do not suggest incomplete code which requires users to modify. Don't use a code block if it's not intended to be executed by the user.\n",
        "If you want the user to save the code in a file before executing it, put # filename: <filename> inside the code block as the first line. Don't include multiple code blocks in one response. Do not ask users to copy and paste the result. Instead, use 'print' function for the output when relevant. Check the execution result returned by the user.\n",
        "If the result indicates there is an error, fix the error and output the code again. Suggest the full code instead of partial code or code changes. If the error can't be fixed or if the task is not solved even after the code is executed successfully, analyze the problem, revisit your assumption, collect additional info you need, and think of a different approach to try.\n",
        "When you find an answer, verify the answer carefully. Include verifiable evidence in your response if possible.\n",
        "Reply 'TERMINATE' in the end when everything is done.\n",
        "3. \"The plant nursery struggles with managing inventory effectively,\n",
        "leading to last-minute scrambles to fulfill orders.\n",
        "This is due to the fragmented nature of data across QuickBooks, emails, and spreadsheets,\n",
        "which makes it challenging to have real-time insights into inventory levels and\n",
        "upcoming order requirements. Business uses quickbooks, gmail and excel.\"\n",
        "4.\"What tools, libraries or frameworks exist to help me make this happen?\n",
        "Preferably using python in a jupyter notebook environment.\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "business_code_writer_agent = ConversableAgent(\n",
        "    \"business_code_writer_agent\",\n",
        "    system_message=business_code_writer_system_message,\n",
        "    llm_config={\"config_list\": [{\"model\": \"gpt-4\", \"api_key\": os.environ[\"OPENAI_API_KEY\"]}]},\n",
        "    code_execution_config=False,  # Turn off code execution for this agent.\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "JdW2_PKPR8vJ",
        "outputId": "53e355a4-a2a5-40ba-e4ea-12bdb92b5b99"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "code_executor_agent (to business_code_writer_agent):\n",
            "\n",
            "Help solve the plant nursery problems in your instructions please\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            ">>>>>>>> USING AUTO REPLY...\n",
            "business_code_writer_agent (to code_executor_agent):\n",
            "\n",
            "To solve the problem of inventory management of plant nursery we need to integrate the data from QuickBooks, emails, and excel spreadsheets and make it available in real-time. This can be achieved by building an ETL (extract, transform, load) pipeline in Python. \n",
            "\n",
            "Here are some of the Python libraries that can be used:\n",
            "\n",
            "1. Pandas: For data manipulation and analysis. It can read data from Excel files, JSON files etc.\n",
            "\n",
            "2. Quickbooks (Python library): To interact with the QuickBooks API and extract data.\n",
            "\n",
            "3. Google API Python Client, Google Auth libraries: To interact with Gmail API and extract data.\n",
            "\n",
            "4. Openpyxl: To read/write Excel 2010 xlsx/xlsm/xltx/xltm files.\n",
            "\n",
            "Plan:\n",
            "\n",
            "- Extract data from Quickbooks using Quickbooks Python library.\n",
            "- Extract data from Gmail using Google API Python Client, Google Auth libraries.\n",
            "- Read Excel using Pandas or openpyxl.\n",
            "- Process and clean the data using Pandas.\n",
            "- Load cleaned data into a new data format which is easier to analyze.\n",
            "\n",
            "Unfortunately, I can't write the actual Python scripts here because this would involve sensitive information like API keys, email credentials etc. Furthermore, the specific operations to perform on the data (like the transformations in the ETL pipeline) will depend heavily on the exact structure and format of your data, which I don't know. \n",
            "\n",
            "However, I can help guide you in learning how to use these libraries if you'd like. Do you want guidance on how to learn these libraries?\n",
            "\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "chat_result = code_executor_agent.initiate_chat(\n",
        "    business_code_writer_agent,\n",
        "    message=\"Help solve the plant nursery problems in your instructions please\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Business scenario:\n",
        "A plant nursery struggles with fragmented data across multiple platforms including QuickBooks for financial transactions, Gmail for communication regarding orders, and Excel spreadsheets for inventory tracking. This fragmentation leads to inefficiencies and challenges in managing inventory accurately, causing last-minute scrambles to fulfill orders."
      ],
      "metadata": {
        "id": "ZyZO1AT_LAjb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Role of AutoGen\n",
        "AutoGen, typically designed for automation in code generation and data handling, can be utilized here to create an integrated ETL (Extract, Transform, Load) pipeline. This pipeline will automate the extraction of data from different sources, transform the data into a standardized format, and load it into a central system where real-time inventory analysis can be performed."
      ],
      "metadata": {
        "id": "Wy7sGQcmMdbQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#How the use of AutoGen will improve or solve the business problem\n",
        "Automation of data extraction, help transform data to maintain consistency, centralize the data , help schedule updates"
      ],
      "metadata": {
        "id": "SRMxqUbBOj18"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#potential challenges\n",
        "Integration of different data sources will require different libraries and APIs."
      ],
      "metadata": {
        "id": "aNJY8KC-O1kI"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMTjVpYp3DG2iAYUWhqFnvh",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}